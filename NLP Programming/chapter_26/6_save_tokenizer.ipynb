{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": ["from keras.preprocessing.text import Tokenizer\nfrom pickle import dump\n\n# load doc into memory\ndef load_doc(filename):\n\t# open the file as read only\n\tfile = open(filename, 'r')\n\t# read all text\n\ttext = file.read()\n\t# close the file\n\tfile.close()\n\treturn text\n\n# load a pre-defined list of photo identifiers\ndef load_set(filename):\n\tdoc = load_doc(filename)\n\tdataset = list()\n\t# process line by line\n\tfor line in doc.split('\\n'):\n\t\t# skip empty lines\n\t\tif len(line) < 1:\n\t\t\tcontinue\n\t\t# get the image identifier\n\t\tidentifier = line.split('.')[0]\n\t\tdataset.append(identifier)\n\treturn set(dataset)\n\n# load clean descriptions into memory\ndef load_clean_descriptions(filename, dataset):\n\t# load document\n\tdoc = load_doc(filename)\n\tdescriptions = dict()\n\tfor line in doc.split('\\n'):\n\t\t# split line by white space\n\t\ttokens = line.split()\n\t\t# split id from description\n\t\timage_id, image_desc = tokens[0], tokens[1:]\n\t\t# skip images not in the set\n\t\tif image_id in dataset:\n\t\t\t# create list\n\t\t\tif image_id not in descriptions:\n\t\t\t\tdescriptions[image_id] = list()\n\t\t\t# wrap description in tokens\n\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n\t\t\t# store\n\t\t\tdescriptions[image_id].append(desc)\n\treturn descriptions\n\n# covert a dictionary of clean descriptions to a list of descriptions\ndef to_lines(descriptions):\n\tall_desc = list()\n\tfor key in descriptions.keys():\n\t\t[all_desc.append(d) for d in descriptions[key]]\n\treturn all_desc\n\n# fit a tokenizer given caption descriptions\ndef create_tokenizer(descriptions):\n\tlines = to_lines(descriptions)\n\ttokenizer = Tokenizer()\n\ttokenizer.fit_on_texts(lines)\n\treturn tokenizer\n\n# load training dataset\nfilename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\n# descriptions\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n# prepare tokenizer\ntokenizer = create_tokenizer(train_descriptions)\n# save the tokenizer\ndump(tokenizer, open('tokenizer.pkl', 'wb'))"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}