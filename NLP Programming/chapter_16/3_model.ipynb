{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": ["from pickle import load\nfrom numpy import array\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nfrom keras.layers import Embedding\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers.merge import concatenate\n\n# load a clean dataset\ndef load_dataset(filename):\n\treturn load(open(filename, 'rb'))\n\n# fit a tokenizer\ndef create_tokenizer(lines):\n\ttokenizer = Tokenizer()\n\ttokenizer.fit_on_texts(lines)\n\treturn tokenizer\n\n# calculate the maximum document length\ndef max_length(lines):\n\treturn max([len(s.split()) for s in lines])\n\n# encode a list of lines\ndef encode_text(tokenizer, lines, length):\n\t# integer encode\n\tencoded = tokenizer.texts_to_sequences(lines)\n\t# pad encoded sequences\n\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n\treturn padded\n\n# define the model\ndef define_model(length, vocab_size):\n\t# channel 1\n\tinputs1 = Input(shape=(length,))\n\tembedding1 = Embedding(vocab_size, 100)(inputs1)\n\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n\tdrop1 = Dropout(0.5)(conv1)\n\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n\tflat1 = Flatten()(pool1)\n\t# channel 2\n\tinputs2 = Input(shape=(length,))\n\tembedding2 = Embedding(vocab_size, 100)(inputs2)\n\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n\tdrop2 = Dropout(0.5)(conv2)\n\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n\tflat2 = Flatten()(pool2)\n\t# channel 3\n\tinputs3 = Input(shape=(length,))\n\tembedding3 = Embedding(vocab_size, 100)(inputs3)\n\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n\tdrop3 = Dropout(0.5)(conv3)\n\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n\tflat3 = Flatten()(pool3)\n\t# merge\n\tmerged = concatenate([flat1, flat2, flat3])\n\t# interpretation\n\tdense1 = Dense(10, activation='relu')(merged)\n\toutputs = Dense(1, activation='sigmoid')(dense1)\n\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n\t# compile\n\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\t# summarize\n\tmodel.summary()\n\tplot_model(model, show_shapes=True, to_file='model.png')\n\treturn model\n\n# load training dataset\ntrainLines, trainLabels = load_dataset('train.pkl')\n# create tokenizer\ntokenizer = create_tokenizer(trainLines)\n# calculate max document length\nlength = max_length(trainLines)\nprint('Max document length: %d' % length)\n# calculate vocabulary size\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary size: %d' % vocab_size)\n# encode data\ntrainX = encode_text(tokenizer, trainLines, length)\n# define model\nmodel = define_model(length, vocab_size)\n# fit model\nmodel.fit([trainX,trainX,trainX], array(trainLabels), epochs=7, batch_size=16)\n# save the model\nmodel.save('model.h5')\n\n"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}