{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.4.2"
    },
    "colab": {
      "name": "09_load_vocab.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asheta66/Machine-Learning-2020/blob/main/NLP%20Programming/chapter_09/09_load_vocab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xFZ2U_zMjIf"
      },
      "source": [
        "# Acquireing dataset\r\n",
        "!git clone https://github.com/asheta66/Machine-Learning-2020\r\n",
        "%cd Machine-Learning-2020/Datasets/NLP\r\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5iY3jvyJ5qB",
        "outputId": "09faf581-07ae-4758-9bfe-3bbef70a9942",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import random\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# load vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "\n",
        "for i, val in enumerate(random.sample(vocab, 40)):\n",
        "  print(val)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ed\n",
            "raises\n",
            "legendary\n",
            "commodus\n",
            "instrument\n",
            "politics\n",
            "onscreen\n",
            "appreciate\n",
            "pump\n",
            "breeding\n",
            "meetings\n",
            "ants\n",
            "tee\n",
            "dreamy\n",
            "gorilla\n",
            "precedes\n",
            "toni\n",
            "round\n",
            "sexist\n",
            "eliminate\n",
            "burned\n",
            "express\n",
            "yeah\n",
            "patriot\n",
            "outside\n",
            "spared\n",
            "click\n",
            "practices\n",
            "discussion\n",
            "asylum\n",
            "seeks\n",
            "psychlo\n",
            "mingna\n",
            "cinque\n",
            "optimism\n",
            "irony\n",
            "survey\n",
            "statement\n",
            "arrakis\n",
            "starlet\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}