{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": ["from pickle import load\nfrom numpy import argmax\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model\nfrom nltk.translate.bleu_score import corpus_bleu\n\n# load a clean dataset\ndef load_clean_sentences(filename):\n\treturn load(open(filename, 'rb'))\n\n# fit a tokenizer\ndef create_tokenizer(lines):\n\ttokenizer = Tokenizer()\n\ttokenizer.fit_on_texts(lines)\n\treturn tokenizer\n\n# max sentence length\ndef max_length(lines):\n\treturn max(len(line.split()) for line in lines)\n\n# encode and pad sequences\ndef encode_sequences(tokenizer, length, lines):\n\t# integer encode sequences\n\tX = tokenizer.texts_to_sequences(lines)\n\t# pad sequences with 0 values\n\tX = pad_sequences(X, maxlen=length, padding='post')\n\treturn X\n\n# map an integer to a word\ndef word_for_id(integer, tokenizer):\n\tfor word, index in tokenizer.word_index.items():\n\t\tif index == integer:\n\t\t\treturn word\n\treturn None\n\n# generate target given source sequence\ndef predict_sequence(model, tokenizer, source):\n\tprediction = model.predict(source, verbose=0)[0]\n\tintegers = [argmax(vector) for vector in prediction]\n\ttarget = list()\n\tfor i in integers:\n\t\tword = word_for_id(i, tokenizer)\n\t\tif word is None:\n\t\t\tbreak\n\t\ttarget.append(word)\n\treturn ' '.join(target)\n\n# evaluate the skill of the model\ndef evaluate_model(model, sources, raw_dataset):\n\tactual, predicted = list(), list()\n\tfor i, source in enumerate(sources):\n\t\t# translate encoded source text\n\t\tsource = source.reshape((1, source.shape[0]))\n\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n\t\traw_target, raw_src = raw_dataset[i]\n\t\tif i < 10:\n\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n\t\tactual.append(raw_target.split())\n\t\tpredicted.append(translation.split())\n\t# calculate BLEU score\n\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n\n# load datasets\ndataset = load_clean_sentences('english-german-both.pkl')\ntrain = load_clean_sentences('english-german-train.pkl')\ntest = load_clean_sentences('english-german-test.pkl')\n# prepare english tokenizer\neng_tokenizer = create_tokenizer(dataset[:, 0])\neng_vocab_size = len(eng_tokenizer.word_index) + 1\neng_length = max_length(dataset[:, 0])\n# prepare german tokenizer\nger_tokenizer = create_tokenizer(dataset[:, 1])\nger_vocab_size = len(ger_tokenizer.word_index) + 1\nger_length = max_length(dataset[:, 1])\n# prepare data\ntrainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\ntestX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n# load model\nmodel = load_model('model.h5')\n# test on some training sequences\nprint('train')\nevaluate_model(model, trainX, train)\n# test on some test sequences\nprint('test')\nevaluate_model(model, testX, test)"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}