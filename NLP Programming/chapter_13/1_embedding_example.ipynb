{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": ["from numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\n# define documents\ndocs = ['Well done!',\n\t\t'Good work',\n\t\t'Great effort',\n\t\t'nice work',\n\t\t'Excellent!',\n\t\t'Weak',\n\t\t'Poor effort!',\n\t\t'not good',\n\t\t'poor work',\n\t\t'Could have done better.']\n# define class labels\nlabels = array([1,1,1,1,1,0,0,0,0,0])\n# integer encode the documents\nvocab_size = 50\nencoded_docs = [one_hot(d, vocab_size) for d in docs]\nprint(encoded_docs)\n# pad documents to a max length of 4 words\nmax_length = 4\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)\n# define the model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 8, input_length=max_length))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n# summarize the model\nmodel.summary()\n# fit the model\nmodel.fit(padded_docs, labels, epochs=50, verbose=0)\n# evaluate the model\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}